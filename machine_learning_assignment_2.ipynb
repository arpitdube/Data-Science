{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting : When our machine learning model has high accuracy when training but low accuracy when tested using a new data (Low Bias and High Variance). As a result, we don't get the desired resulted from our model.\n",
    "\n",
    "Underfitting : When our machine learning model is poorly trained and performs poorly when tested with test dataset (High Bias and High Variance). As a result, the model lacks the complexity to represent the underlying patterns in the data.\n",
    "\n",
    "Mitigation :-  \n",
    "(a) Overfitting:-\n",
    "-> Use of more training data to reduce the impact of noise.\n",
    "-> Cross-validation to tune hyperparameters and assess model performance on multiple subsets of the data.\n",
    "-> Early stopping, which involves monitoring the model's performance on a validation set and stopping training when performance begins to degrade.  \n",
    "\n",
    "(b) Underfitting:-\n",
    "-> Increase model complexity.\n",
    "-> Add more relevant features to the dataset if possible.\n",
    "-> Ensure data preprocessing and feature engineering are done effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2: How can we reduce overfitting? Explain in brief.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Increasing the size of training dataset can help the model learn the underlying patterns in the data better and reduce the impact of noise.  \n",
    "\n",
    "->  Use a simpler model architecture with fewer parameters.  \n",
    "\n",
    "->  Carefully choosing relevant features and removing irrelevant or redundant ones.  \n",
    "\n",
    "->  Experiment with different hyperparameters to find values that lead to better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3: Explain underfitting. List scenarios where underfitting can occur in ML.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underfitting refers to a situation where our machine learning model is not traiend properly which leads to poor performnace when tested using test data. The model is to simple to capture the underlying pattern in the data. Underfitting can happen in various scenarios:-  \n",
    "1) When you use a model that is inherently too simple for the complexity of the data, such as using linear regression for a problem with highly nonlinear relationships.  \n",
    "\n",
    "2)  If we have a dataset with important features that are not included in the model, the model may underfit because it lacks the necessary information to make accurate predictions.  \n",
    "\n",
    "3) When you have a small amount of training data relative to the complexity of the problem, the model may struggle to find meaningful patterns and could underfit.  \n",
    "\n",
    "4) Algorithms with high bias are prone to underfitting because they make strong assumptions about the data's linearity or simplicity.  \n",
    "\n",
    "5) Using an inappropriate algorithm not suited to the problem.  \n",
    "\n",
    "6) Failing to preprocess and clean the data properly can lead to underfitting, as noise and outliers can affect the model's ability to learn meaningful patterns.  \n",
    "\n",
    "7) Ingnoring the interactions between different features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias-Variance tradeoff in machine learning refers to the balance between two sources of error in predictive models, bias and variance. Bias refers to the error introduced by approximating a real-world problem (which may be complex) by a simplified model. High bias model has low training accuracy. So our model shouldn't be bias. Variance refers to the error introduced by the model's sensitivity to small fluctuations in the training data. A model with high varinace has lower prediction accuracy. A generalized model has low bias and low variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For detecting overfitting:-  \n",
    "1) Plot the model's performance (e.g., accuracy or loss) on both the training and validation datasets as a function of a hyperparameter (e.g., model complexity or regularization strength). Overfitting often shows as a divergence between training and validation curves.  \n",
    "\n",
    "2) Assess model's performnace on multiple subsets of the data.  \n",
    "\n",
    "3) Analyze the importance of features in your model. If some features have very high importance, it may suggest overfitting, especially if those features do not have a clear real-world significance.\n",
    "\n",
    "For detecting underfitting:-\n",
    "1) Similar to detecting overfitting, validation curves can also reveal underfitting. If both training and validation performance are poor, it may indicate underfitting due to a model that is too simple.  \n",
    "\n",
    "2) Visualize the model's predictions and compare them to the ground truth. If the model's predictions do not align well with the actual data patterns, it might be underfitting.  \n",
    "\n",
    "3) Gradually increase the model's complexity by adding more features, layers, or parameters. If the model's performance improves with increased complexity, it suggests that the initial model was underfitting.  \n",
    "\n",
    "4) If some features are consistently assigned low importance by the model, it might suggest that the model is underfitting and not adequately capturing the data's complexity.  \n",
    "\n",
    "5) Review your dataset for any missing or important features that have not been included in the model. Underfitting can occur if relevant information is missing from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Increasing model complexity tends to decrease bias but increase variance, and vice versa. The challenge is to find the right balance for good model performance.  \n",
    "\n",
    "-> High bias models tend to underfit, performing poorly on both training and validation/test data. High variance models tend to overfit, performing well on training data but poorly on validation/test data.  \n",
    "\n",
    "-> Bias corresponds to simpler models, while variance corresponds to more complex models. High bias models make strong simplifying assumptions, while high variance models capture intricate details, often including noise.  \n",
    "\n",
    "-> Bias impacts the model's ability to generalize by oversimplifying the problem, while variance impacts generalization by fitting noise and random variations.  \n",
    "\n",
    "Examples of high bias and high variance models are:-  \n",
    "1) Linear regression model to data with a non-linear relationship. Here, liner regression makes linear assumption about data.  \n",
    "\n",
    "2) A decision tree with very few nodes or a shallow depth may underfit data with intricate decision boundaries. It simplifies the decision-making process too much and struggles to capture complex relationships.  \n",
    "\n",
    "3) Using a low-degree polynomial regression to model data with a higher-degree polynomial relationship will lead to underfitting.  \n",
    "\n",
    "4) When you have a complex problem, and you use a neural network with only a few layers or neurons, it might underfit the data. Simple neural networks may not have the capacity to learn the intricate features and patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Regularization in machine learning is a set of techniques used to prevent overfitting, which occurs when a model is too complex and fits the training data too closely, including the noise and random fluctuations. Regularization introduces additional constraints or penalties on the model's parameters during training, discouraging it from becoming overly complex. This helps improve the model's ability to generalize to unseen data. Some common techinques of regularization are:-  \n",
    "1) L1 Regularization (Lasso): L1 regularization adds a penalty term to the loss function that is proportional to the absolute values of the model's parameters. This encourages the model to set some of the parameters to exactly zero, effectively performing feature selection. L1 regularization is useful when you suspect that some features are irrelevant or redundant, and you want the model to automatically select the most important ones.  \n",
    "\n",
    "2) L2 Regularization (Ridge):  L2 regularization adds a penalty term to the loss function that is proportional to the square of the model's parameters. This discourages the model from assigning extremely large weights to any single feature. L2 regularization is effective when you want to prevent large weights and make the model's predictions more stable and less sensitive to small variations in the data.  \n",
    "\n",
    "3) Elastic Net Regularization: Elastic Net combines both L1 and L2 regularization by adding a linear combination of the L1 and L2 penalty terms to the loss function. It offers a balance between feature selection (L1) and weight stabilization (L2).  Elastic Net is beneficial when you want to address both feature selection and weight stability concerns simultaneously.  \n",
    "\n",
    "4) Dropout: Dropout is a regularization technique commonly used in neural networks. During training, dropout randomly sets a fraction of neurons to zero at each forward and backward pass. This prevents specific neurons from becoming overly reliant on each other and encourages the network to learn more robust features. Dropout is effective in preventing overfitting in deep neural networks.  \n",
    "\n",
    "5) Early Stopping: Early stopping involves monitoring the model's performance on a validation dataset during training. When the performance on the validation set starts to degrade (e.g., the loss increases), training is stopped early. This prevents the model from overfitting the training data. Early stopping is useful in situations where you do not have control over other regularization parameters or when training deep neural networks.  \n",
    "\n",
    "6) Cross-Validation:  Cross-validation involves splitting the data into multiple subsets (folds) and training the model on different subsets while validating on the others. It helps in selecting the best hyperparameters and assessing how well the model generalizes to new data. Cross-validation aids in choosing appropriate regularization strengths and other hyperparameters to prevent overfitting."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
